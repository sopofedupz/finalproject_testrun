{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and clean data as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/Resources'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Resources/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the last column\n",
    "df = df.drop(columns = 'Unnamed: 32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any missing data\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analysis (Descriptives/Statistics Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the data variables\n",
    "\n",
    "# a) radius (mean of distances from center to points on the perimeter)\n",
    "# b) texture (standard deviation of gray-scale values)\n",
    "# c) perimeter\n",
    "# d) area\n",
    "# e) smoothness (local variation in radius lengths)\n",
    "# f) compactness (perimeter^2 / area - 1.0)\n",
    "# g) concavity (severity of concave portions of the contour)\n",
    "# h) concave points (number of concave portions of the contour)\n",
    "# i) symmetry\n",
    "# j) fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id column for further analysis\n",
    "updated_df = df.drop(columns='id')\n",
    "\n",
    "# check dataset\n",
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count no. of benign and malignant tumors\n",
    "count_df = updated_df.groupby('diagnosis')\n",
    "print(count_df['diagnosis'].count())\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# create bar chart\n",
    "ax_bar = sns.countplot(x=\"diagnosis\", data=updated_df, palette=\"Set1\")\n",
    "plt.title(\"Count of Benign and Malignant Tumors\");\n",
    "plt.ylabel(\"Count\");\n",
    "plt.xlabel(\"Diagnosis\");\n",
    "# plt.savefig(f'Resources/Count - tumor diagnosis.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode diagnosis column to 1 (malignant) and 0 (benign)\n",
    "def tumor(row):\n",
    "    if row['diagnosis'] == 'B':\n",
    "        return 0\n",
    "    if row['diagnosis'] == 'M':\n",
    "        return 1\n",
    "    \n",
    "# create a new column with the recoded values\n",
    "updated_df['tumor'] = updated_df.apply (lambda row: tumor(row), axis=1)\n",
    "\n",
    "# calculate correlation coefficients\n",
    "corr_df = updated_df.corr()\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are ~29 features in the data, we have decided to look at the correlation heatmap to determine the features\n",
    "# that we want to include in our classification models\n",
    "\n",
    "# correlation heatmap\n",
    "f,ax1 = plt.subplots(figsize=(20, 20))\n",
    "sns.heatmap(updated_df.corr(), cmap='BuPu',annot=True, linewidths=.5, fmt= '.2f',ax=ax1)\n",
    "plt.xticks(fontsize=11,rotation=70)\n",
    "# plt.savefig(f'Resources/Corr Heatmap.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for Classifical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the y variable\n",
    "y= df['diagnosis'].map({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with selected features based on correlation results (keeping those with coefficient of .5 and above)\n",
    "X = df[['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', \n",
    "        'radius_se', 'perimeter_se', 'area_se', 'radius_worst', 'area_worst', 'perimeter_worst', 'compactness_worst', \n",
    "        'concavity_worst', 'concave points_worst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33, random_state=42)\n",
    "N ,D = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale/normalize the train and test data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to store classifier accuracy\n",
    "classifier_accu = []\n",
    "classifier_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function runs the Predictor. It prints how many inaccurate predictions were made and whether the\n",
    "# model is over predicting or under predicting\n",
    "def Predictor(classifier, name):\n",
    "    if name == 'Sequential Model':\n",
    "        seq_predict = classifier.predict_classes(X_test)\n",
    "        predictions = []\n",
    "        for sublist in seq_predict:\n",
    "            for item in sublist:\n",
    "                predictions.append(item)\n",
    "        train_test_score = classifier.evaluate(X_test,y_test)\n",
    "        score = train_test_score[1]\n",
    "    else:\n",
    "        predictions = classifier.predict(X_test)\n",
    "        score = classifier.score(X_test, y_test)\n",
    "    \n",
    "    predictions_df = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).reset_index(drop=True)\n",
    "    predictions_df[\"Sum\"] = predictions_df.sum(axis=1)\n",
    "    predictions_df = predictions_df[predictions_df.Sum == 1]\n",
    "    \n",
    "    inaccurate_predictions = len(predictions_df)\n",
    "    false_positives = predictions_df[\"Prediction\"].sum()\n",
    "    false_negatives = predictions_df[\"Actual\"].sum()\n",
    "    difference = false_positives - false_negatives\n",
    "\n",
    "    results_string = (f\"Results for {name} classifier:\\n\"\n",
    "                      f\"-------------------------------------\\n\"\n",
    "                      f\"Score: {score}\\n\"\n",
    "                      f\"Inaccurate Predictions: {inaccurate_predictions}\\n\"\n",
    "                      f\"False Positives: {false_positives}\\n\"\n",
    "                      f\"False Negatives: {false_negatives}\\n\"\n",
    "                      f\"Difference (positive is good): {difference}\\n\")\n",
    "    print(results_string)\n",
    "    \n",
    "#     text_file = open(f'Resources/Results/{name}.txt', 'w')\n",
    "#     text_file.write(results_string)\n",
    "#     text_file.close()\n",
    "    \n",
    "    # plot confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, predictions)\n",
    "    group_names = ['TN', 'FP', 'FN', 'TP']\n",
    "    group_counts = [\"{:.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    categories = ['Benign', 'Malignant']\n",
    "    sns.heatmap(cf_matrix, fmt = '', annot = labels, xticklabels = categories, yticklabels = categories, cmap = 'BuPu')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.title(name)\n",
    "#     plt.savefig(f'Resources/Results/{name}.jpg', dpi=300)\n",
    "    # plot ROC curves\n",
    "    fpr, tpr, _ = roc_curve(predictions,y_test)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(name)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "#     plt.savefig(f'Resources/Results/{name}2.jpg', dpi=300)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "log_reg_accu = logreg.score(X_test, y_test)\n",
    "classifier_accu.append(log_reg_accu)\n",
    "classifier_list.append(\"Logistic Regression\")\n",
    "\n",
    "print(f\"Training Data Score: {logreg.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {logreg.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', \n",
    "        'radius_se', 'perimeter_se', 'area_se', 'radius_worst', 'area_worst', 'perimeter_worst', 'compactness_worst', \n",
    "        'concavity_worst', 'concave points_worst']\n",
    "\n",
    "# get importance\n",
    "importance = logreg.coef_[0]\n",
    "\n",
    "# summarize feature importance\n",
    "for i,(v1,v2) in enumerate(zip(importance,features)):\n",
    "    print(f\"Feature %0d: {v2}, Score: %.5f\" % (i,v1))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "# plt.savefig(f'Resources/LogReg - Features Importance.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictor(logreg, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = updated_df['diagnosis']\n",
    "target_names = ['M', 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_data = updated_df.drop('diagnosis', axis=1)\n",
    "feature_names = svm_data.columns\n",
    "svm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Support vector machine linear classifier\n",
    "from sklearn.svm import SVC \n",
    "vector = SVC(kernel='linear')\n",
    "vector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy\n",
    "\n",
    "svc_accu = vector.score(X_test, y_test)\n",
    "classifier_accu.append(svc_accu)\n",
    "classifier_list.append(\"SVM\")\n",
    "\n",
    "print('Test Acc: %.3f' % vector.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = vector.predict(X_test)\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictor(vector, 'Support Vector Machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "clf_accu = clf.score(X_test, y_test)\n",
    "classifier_accu.append(clf_accu)\n",
    "classifier_list.append(\"Decision Tree\")\n",
    "\n",
    "clf_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictor(clf, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "\n",
    "rf_accu = rf.score(X_test, y_test)\n",
    "classifier_accu.append(rf_accu)\n",
    "classifier_list.append(\"Random Forest\")\n",
    "\n",
    "rf_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(rf.feature_importances_, feature_names), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictor(rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 20, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_score = knn.score(X_train, y_train)\n",
    "    test_score = knn.score(X_test, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}\")\n",
    "    \n",
    "plt.plot(range(1, 20, 2), train_scores, marker='o')\n",
    "plt.plot(range(1, 20, 2), test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "# plt.savefig(f'Resources/Results/KNN.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_accu = knn.score(X_test, y_test)\n",
    "classifier_accu.append(knn_accu)\n",
    "classifier_list.append(\"KNN\")\n",
    "\n",
    "print('k=3 Test Acc: %.3f' % knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictor(knn, \"K Nearest Neighbor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now creating the model\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Input(shape=(D,)),\n",
    "                                    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can avoid this error by converting your labels to arrays before calling model.fit()\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r= model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test score\n",
    "\n",
    "seq_accu = model.evaluate(X_test, y_test)\n",
    "seq_accu = seq_accu[1]\n",
    "classifier_accu.append(seq_accu)\n",
    "classifier_list.append(\"Sequential Model\")\n",
    "\n",
    "print(\"Train score\",model.evaluate(X_train,y_train))\n",
    "print(\"Test score\",model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r.history['loss'],label='loss')\n",
    "plt.plot(r.history['val_loss'],label='val_loss')\n",
    "plt.legend()\n",
    "# plt.savefig(f'Resources/Sequential.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictor(model, 'Sequential Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall view of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictor(classifier, \"Logistic Regression\") <-- needs to be updated with correct classifier\n",
    "#Predictor(classifier, \"Support Vector Machines\") <-- needs to be updated with correct classifier\n",
    "#Predictor(clf, \"Decision Tree\")\n",
    "#Predictor(rf, \"Random Forest\")\n",
    "#Predictor(knn, \"K Nearest Neighbor\")\n",
    "#Predictor(vector, \"Support Vector Machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to add svm\n",
    "\n",
    "# creating a roc chart with all classifiers\n",
    "\n",
    "lr_predictions = logreg.predict(X_test)\n",
    "clf_predictions = clf.predict(X_test)\n",
    "rf_predictions = rf.predict(X_test)\n",
    "knn_predictions = knn.predict(X_test)\n",
    "svm_predictions = vector.predict(X_test)\n",
    "\n",
    "seq_predict = model.predict_classes(X_test)\n",
    "seq_predictions = []\n",
    "for sublist in seq_predict:\n",
    "    for item in sublist:\n",
    "        seq_predictions.append(item)\n",
    "\n",
    "lr_fpr, lr_tpr, _ = roc_curve(lr_predictions,y_test)\n",
    "clf_fpr, clf_tpr, _ = roc_curve(clf_predictions,y_test)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(rf_predictions,y_test)\n",
    "knn_fpr, knn_tpr, _ = roc_curve(knn_predictions,y_test)\n",
    "svm_fpr, svm_tpr, _ = roc_curve(svm_predictions,y_test)\n",
    "seq_fpr, seq_tpr, _ = roc_curve(seq_predictions,y_test)\n",
    "\n",
    "lr_roc_auc = auc(lr_fpr, lr_tpr)\n",
    "clf_roc_auc = auc(clf_fpr, clf_tpr)\n",
    "rf_roc_auc = auc(rf_fpr, rf_tpr)\n",
    "knn_roc_auc = auc(knn_fpr, knn_tpr)\n",
    "svm_roc_auc = auc(svm_fpr, svm_tpr)\n",
    "seq_roc_auc = auc(seq_fpr, seq_tpr)\n",
    "\n",
    "plt.subplots(1, figsize=(10,10))\n",
    "plt.plot(lr_fpr, lr_tpr, color='darkorange',\n",
    "         lw=1, label='Logistic Regression (area = %0.2f)' % lr_roc_auc)\n",
    "plt.plot(clf_fpr, clf_tpr, color='darkgreen',\n",
    "         lw=1, label='Decision Tree (area = %0.2f)' % clf_roc_auc)\n",
    "plt.plot(rf_fpr, rf_tpr, color='purple',\n",
    "         lw=1, label='Random Forest (area = %0.2f)' % rf_roc_auc)\n",
    "plt.plot(knn_fpr, knn_tpr, color='red',\n",
    "         lw=1, label='K Nearest Neighbor (area = %0.2f)' % knn_roc_auc)\n",
    "plt.plot(svm_fpr, svm_tpr, color ='pink',\n",
    "        lw=1, label='Support Vector Machine (area = %0.2f)' % svm_roc_auc)\n",
    "plt.plot(seq_fpr, seq_tpr, color ='blue',\n",
    "        lw=1, label='Sequential Model (area = %0.2f)' % seq_roc_auc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.savefig(f'Resources/ROC - ALL models.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe with classifier accuracy\n",
    "accu_df = pd.DataFrame(list(zip(classifier_list, classifier_accu)), \n",
    "               columns =['Classifier', 'Accuracy']) \n",
    "accu_df.sort_values(by='Accuracy', ascending=False, inplace=True)\n",
    "\n",
    "#write html to file\n",
    "accu_df.to_html(\"templates/table.html\", index=False, header=True)\n",
    "accu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bar chart\n",
    "ax_accu_bar = accu_df.plot.bar(x='Classifier', y='Accuracy', rot=45, legend=False)\n",
    "plt.title(\"Classifiers Accuracy\");\n",
    "plt.ylabel(\"Accuracy\");\n",
    "plt.xlabel(\"Classification Models\");\n",
    "plt.ylim(0.85,1)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'Resources/Classifiers Accuracy.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
